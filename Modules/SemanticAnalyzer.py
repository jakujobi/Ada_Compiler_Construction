#!/usr/bin/env python3
# SemanticAnalyzer.py
# Author: John Akujobi
# GitHub: https://github.com/jakujobi/Ada_Compiler_Construction
# Date: 2025-03-31
# Version: 1.0
"""
Semantic Analyzer for Ada Compiler

This module performs semantic analysis on the parse tree generated by the parser.
It performs the following semantic actions:
- Insert constants, variables, and procedures into the symbol table
- Set the value or type for constants and variables, including size and current offset
- Track the size of local variables and parameters for procedures
- Reject multiple declarations of the same name at the same depth
- Print the contents of the symbol table upon exiting each procedure and at the end

The semantic analyzer walks the parse tree and performs the appropriate semantic actions
based on the grammar productions. It reports semantic errors and continues analysis
when possible, or stops at the first error if configured to do so.
"""

import os
import sys
from typing import List, Dict, Optional, Any, Tuple
from prettytable import PrettyTable

# Add the parent directory to the path so we can import modules
repo_home_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.append(repo_home_path)

from Modules.Token import Token
from Modules.Definitions import Definitions
from Modules.RDParser import ParseTreeNode
from Modules.AdaSymbolTable import AdaSymbolTable, VarType, EntryType, ParameterMode, Parameter, TableEntry
from Modules.Logger import Logger


class SemanticAnalyzer:
    """
    Semantic analyzer for Ada compiler.
    
    This class walks the parse tree and performs semantic actions according to
    the Ada grammar, including symbol table management, type checking, and 
    tracking of variable offsets.
    """
    
    def __init__(self, symbol_table: AdaSymbolTable, stop_on_error: bool = False, logger: Logger = None):
        """
        Initialize the semantic analyzer.
        
        Args:
            symbol_table: The symbol table to use for semantic analysis
            stop_on_error: Whether to stop at the first error or continue
            logger: An optional logger instance for logging errors and information
        """
        self.symbol_table = symbol_table
        self.stop_on_error = stop_on_error
        self.logger = logger if logger else Logger()
        self.errors = []
        self.current_depth = 0
        self.current_offset = 0
        
        # For managing variable memory allocation
        self.depth_offsets = {0: 0}  # Track offset for each depth level
        
        # Define sizes for each variable type
        self.type_sizes = {
            VarType.INT: 2,      # Integers have size 2
            VarType.CHAR: 1,     # Characters have size 1
            VarType.FLOAT: 4,    # Floats have size 4
            VarType.REAL: 4      # Alias for FLOAT
        }
        
        # Define mapping from token types to variable types
        self.token_to_var_type = {
            "INTEGERT": VarType.INT,
            "REALT": VarType.FLOAT,
            "CHART": VarType.CHAR,
            "FLOAT": VarType.FLOAT
        }
        
        self.logger.info("Semantic Analyzer initialized")
        self.logger.debug(f"Symbol table size: {symbol_table.table_size}, Stop on error: {stop_on_error}")
    
    def analyze_parse_tree(self, parse_tree_root: ParseTreeNode) -> bool:
        """
        Analyze the parse tree and perform semantic actions.
        
        Args:
            parse_tree_root: The root of the parse tree from the parser
            
        Returns:
            True if analysis was successful, False otherwise
        """
        self.logger.info("Starting semantic analysis of parse tree")
        if not parse_tree_root:
            self.report_error("No parse tree to analyze", line=0, column=0)
            return False
        else:
            self.logger.info("There is a parse tree to analyze")
            
        # Start analysis at the program level
        result = self.analyze_prog(parse_tree_root)
        
        # At the end of analysis, print all entries remaining at depth 0.
        print("\nRemaining global entries at depth 0:")
        self.print_symbol_table(0)
        
        self.logger.info(f"Semantic analysis complete with {len(self.errors)} errors")
        # Return success only if no errors were found
        return result and len(self.errors) == 0
    
    def analyze_prog(self, node: ParseTreeNode) -> bool:
        """
        Analyze a Prog node in the parse tree.
        
        Grammar rule: Prog -> procedure idt Args is DeclarativePart Procedures begin SeqOfStatements end idt;
        
        Actions:
        1. Enter a new scope (increment depth)
        2. Insert procedure name into symbol table at parent depth
        3. Process arguments (if any)
        4. Process declarative part (variable/constant declarations)
        5. Process nested procedures
        6. Process statements
        7. Exit scope (decrement depth and print symbol table)
        
        Args:
            node: The Prog node to analyze
            
        Returns:
            True if analysis succeeded, False if critical errors were found
        """
        self.logger.info(f"Analyzing program node of type '{node.name}'")
        if node.name != "Prog":
            line, column = self.get_location_info(node)
            self.report_error(f"Expected 'Prog' node, got '{node.name}'", line, column)
            return False
            
        # Navigate through the procedure structure
        # Format: procedure idt Args is DeclarativePart Procedures begin SeqOfStatements end idt;
        
        # Get the procedure name (second child should be the identifier)
        if len(node.children) < 2 or node.children[1].name != "ID":  # Changed from "idt" to "ID"
            line, column = self.get_location_info(node.children[0] if node.children else node)
            self.report_error("Missing procedure identifier", line, column)
            return False
            
        proc_name_node = node.children[1]
        proc_name = proc_name_node.token.lexeme
        self.logger.info(f"Processing procedure '{proc_name}' at depth {self.current_depth}")
        
        # Check if procedure name already exists at the current depth
        existing_entry = self.symbol_table.lookup(proc_name, self.current_depth)
        if existing_entry:
            line, column = self.get_location_info(proc_name_node)
            self.report_error(f"Duplicate declaration of procedure '{proc_name}'", line, column)
            return False
            
        # Insert procedure into symbol table at current depth
        self.logger.info(f"Inserting procedure '{proc_name}' into symbol table at depth {self.current_depth}")
        proc_entry = self.symbol_table.insert(proc_name, proc_name_node.token.token_type, self.current_depth)
        
        # Set initial procedure info (will update later)
        self.logger.debug(f"Initializing procedure info for '{proc_name}'")
        proc_entry.set_procedure_info(0, 0, None, [])
        proc_entry.entry_type = EntryType.PROCEDURE
        
        # Enter new scope for procedure body
        self.logger.debug(f"Entering scope for procedure '{proc_name}', new depth: {self.current_depth + 1}")
        self.current_depth += 1
        
        # Reset the offset for the new scope
        self.logger.debug(f"Resetting offset for new scope at depth {self.current_depth}")
        self.depth_offsets[self.current_depth] = 0
        self.current_offset = 0
        
        # Process procedure arguments (if any)
        self.logger.debug("Processing procedure arguments")
        args_node = self.find_child_by_name(node, "Args")
        if args_node:
            param_info = self.analyze_args(args_node)
            if param_info:
                param_count, param_list = param_info
                # Update procedure entry with formal parameter info;
                # Formal parameters are inserted at the new scope (depth + 1)
                self.logger.debug(f"Updating procedure entry for '{proc_name}' with parameter count {param_count}")
                proc_entry.param_count = param_count
                proc_entry.param_list = param_list
        
        # Process declarative part (variable/constant declarations)
        decl_part_node = self.find_child_by_name(node, "DeclarativePart")
        if decl_part_node:
            self.logger.debug("Processing declarative part")
            self.analyze_declarative_part(decl_part_node)
        
        # Process nested procedures
        procedures_node = self.find_child_by_name(node, "Procedures")
        if procedures_node:
            self.logger.debug("Processing nested procedures")
            self.analyze_procedures(procedures_node)
        
        # Process statements
        seq_of_statements_node = self.find_child_by_name(node, "SeqOfStatements")
        if seq_of_statements_node:
            self.logger.debug("Processing statements")
            self.analyze_seq_of_statements(seq_of_statements_node)
        
        # Update procedure entry with final local size
        self.logger.debug(f"Updating procedure entry for '{proc_name}' with local size {self.depth_offsets[self.current_depth]}")
        proc_entry.local_size = self.depth_offsets[self.current_depth]
        
        # Print symbol table for this scope
        self.logger.debug(f"Exiting procedure '{proc_name}' (depth {self.current_depth}):")
        print(f"\nExiting procedure '{proc_name}' (depth {self.current_depth}):")
        self.print_symbol_table(self.current_depth)
        
        # Delete entries at this depth as we exit the scope
        self.logger.debug(f"Deleting entries at depth {self.current_depth}")
        self.symbol_table.deleteDepth(self.current_depth)
        
        # Exit scope
        self.logger.debug(f"Exiting scope for procedure '{proc_name}'")
        self.current_depth -= 1
        
        # Restore parent scope's offset
        self.current_offset = self.depth_offsets[self.current_depth]
        
        self.logger.info(f"Completed analysis of procedure '{proc_name}'")
        return True
    
    def analyze_declarative_part(self, node: ParseTreeNode) -> bool:
        """
        Analyze a DeclarativePart node in the parse tree.
        
        Grammar rule: DeclarativePart -> IdentifierList : TypeMark ; DeclarativePart | ε
        
        Actions:
        1. Process identifier list (get all variable/constant names)
        2. Process type mark (get type information for the variables/constants)
        3. Insert each identifier into the symbol table with the appropriate type info
        4. Recursively process the rest of the declarative part
        
        Args:
            node: The DeclarativePart node to analyze
            
        Returns:
            True if analysis succeeded, False if critical errors were found
        """
        self.logger.info(f"Analyzing declarative part at depth {self.current_depth}")
        # If the node is an ε-production, ignore further processing.
        if len(node.children) == 1 and node.children[0].name == "ε":
            self.logger.debug("DeclarativePart is ε (empty), skipping processing")
            return True
            
        if node.name != "DeclarativePart" or not node.children:
            # Empty declarative part (epsilon production)
            self.logger.debug("Empty declarative part")
            return True
            
        # The first child should be IdentifierList
        self.logger.debug("Processing IdentifierList")
        id_list_node = self.find_child_by_name(node, "IdentifierList")
        if not id_list_node:
            line, column = self.get_location_info(node.children[0] if node.children else node)
            self.report_error("Missing identifier list in declarative part", line, column)
            return False
            
        # Get list of identifiers
        self.logger.debug("Getting list of identifiers")
        identifiers = self.analyze_identifier_list(id_list_node)
        self.logger.debug(f"Found {len(identifiers)} identifiers: {[id.lexeme for id in identifiers]}")
        
        # Next should be a colon
        self.logger.debug("Processing colon")
        colon_index = self.find_child_index_by_token_type(node, "COLON")
        if colon_index == -1:  # Removed extra length check
            line, column = self.get_location_info(id_list_node)
            self.report_error("Missing colon in declarative part", line, column)
            return False
            
        # Next should be TypeMark
        self.logger.debug("Processing TypeMark")
        type_mark_node = self.find_child_by_name(node, "TypeMark") 
        if not type_mark_node:
            line, column = self.get_location_info(node.children[colon_index])
            self.report_error("Missing type mark in declarative part", line, column)
            return False
            
        # Process type mark to get variable type or constant info
        self.logger.debug("Processing type mark")
        type_info = self.analyze_type_mark(type_mark_node)
        if not type_info:
            return False
            
        self.logger.debug(f"Type info: is_constant={type_info[0]}, type={type_info[1]}, value={type_info[2]}")
        
        # Determine if this is a variable or constant declaration
        self.logger.debug("Determining if this is a variable or constant declaration")
        is_constant, var_type, const_value = type_info
        
        # Insert each identifier into the symbol table
        for identifier in identifiers:
            # Check for duplicate declaration at current depth
            self.logger.debug(f"Checking for duplicate declaration of '{identifier.lexeme}'")
            existing_entry = self.symbol_table.lookup(identifier.lexeme, self.current_depth)
            if existing_entry:
                line, column = self.get_location_info(identifier)
                self.report_error(f"Duplicate declaration of '{identifier.lexeme}'", line, column)
                continue
                
            # Insert the identifier into the symbol table
            self.logger.debug(f"Inserting '{identifier.lexeme}' into the symbol table")
            if not is_constant:
                size = self.type_sizes.get(var_type, 0)
                entry = self.symbol_table.insert(identifier.lexeme, identifier.token_type, self.current_depth)
                entry.set_variable_info(var_type, self.current_offset, size)
                entry.entry_type = EntryType.VARIABLE
                self.current_offset += size
                self.depth_offsets[self.current_depth] = self.current_offset
            else:
                entry = self.symbol_table.insert(identifier.lexeme, identifier.token_type, self.current_depth)
                entry.set_constant_info(var_type, const_value)
                entry.entry_type = EntryType.CONSTANT
        
        # Process the rest of the declarative part (if any)
        self.logger.debug("Processing remaining declarative part")
        semicolon_index = self.find_child_index_by_token_type(node, "SEMICOLON")
        if semicolon_index != -1 and semicolon_index < len(node.children) - 1:
            next_decl_part = node.children[semicolon_index + 1]
            self.logger.debug("Checking for next declarative part")
            if next_decl_part.name == "DeclarativePart":
                self.analyze_declarative_part(next_decl_part)
        
        self.logger.debug(f"Processed all declarations in declarative part")
        return True
    
    def analyze_identifier_list(self, node: ParseTreeNode) -> List[Token]:
        """
        Analyze an IdentifierList node in the parse tree.
        
        Grammar rule: IdentifierList -> idt | IdentifierList , idt
        
        Args:
            node: The IdentifierList node to analyze
            
        Returns:
            List of identifier tokens in the list
        """
        self.logger.debug(f"Analyzing identifier list node: {node.name}")
        identifiers = []
        current_node = node
        
        while current_node:
            # Look for the identifier token in the current node
            self.logger.debug("Looking for identifier token")
            idt_node = None
            for child in current_node.children:
                if child.name == "ID":  # Changed from "idt" to "ID"
                    idt_node = child
                    identifiers.append(child.token)
                    break
            
            # Check if there's another IdentifierList (for comma-separated lists)
            next_list_node = self.find_child_by_name(current_node, "IdentifierList")
            if next_list_node:
                current_node = next_list_node
            else:
                break
                
        self.logger.debug(f"Found {len(identifiers)} identifiers: {[id.lexeme for id in identifiers]}")
        return identifiers
    
    def analyze_type_mark(self, node: ParseTreeNode) -> Optional[Tuple[bool, VarType, Any]]:
        """
        Analyze a TypeMark node in the parse tree.
        
        Grammar rule: TypeMark -> integert | realt | chart | float | const assignop Value
        
        Args:
            node: The TypeMark node to analyze
            
        Returns:
            Tuple of (is_constant, variable_type, constant_value) or None if error
        """
        self.logger.debug(f"Analyzing type mark node: {node.name}")
        if not node.children:
            line, column = self.get_location_info(node)
            self.report_error("Empty type mark", line, column)
            return None
            
        # Get the first child which should be the type
        self.logger.debug("Getting first child")
        type_node = node.children[0]
        self.logger.debug(f"Type node: {type_node.name}, has token: {type_node.token is not None}")
        
        # Check if this is a constant declaration
        self.logger.debug("Checking if this is a constant declaration")
        if type_node.name == "const" or (type_node.token and type_node.token.token_type == "CONSTANT"):
            # Format: const assignop Value
            if len(node.children) < 3:
                line, column = self.get_location_info(type_node)
                self.report_error("Invalid constant declaration, missing value", line, column)
                return None
                
            # Check for assignment operator
            assign_node = node.children[1]
            if assign_node.name != "assignop" and (assign_node.token and assign_node.token.token_type != "ASSIGN"):
                line, column = self.get_location_info(type_node)
                self.report_error("Invalid constant declaration, missing assignment operator", line, column)
                return None
                
            # Get the value node
            value_node = self.find_child_by_name(node, "Value")
            if not value_node or not value_node.children:
                line, column = self.get_location_info(node.children[1])
                self.report_error("Invalid constant declaration, missing value", line, column)
                return None
                
            # Extract the value
            value_token = value_node.children[0].token
            
            # Determine the constant type based on the token type
            if value_token.token_type.name in ("NUM", "INTLIT"):
                const_type = VarType.INT
                value = int(value_token.lexeme)
            elif value_token.token_type.name in ("REAL", "FLOATLIT"):
                const_type = VarType.FLOAT
                value = float(value_token.lexeme)
            elif value_token.token_type.name == "CHRLIT":
                const_type = VarType.CHAR
                value = value_token.lexeme.strip("'")
            else:
                line, column = self.get_location_info(value_token)
                self.report_error(f"Invalid constant value type: {value_token.token_type.name}", line, column)
                return None
            
            self.logger.debug(f"Constant value determined: type={const_type}, value={value}")
            return (True, const_type, value)
        else:
            # Variable type declaration
            self.logger.debug("Processing variable type declaration")
            if type_node.token and type_node.token.token_type in self.token_to_var_type:
                var_type = self.token_to_var_type[type_node.token.token_type]
            elif type_node.name.lower() == "integert":
                var_type = VarType.INT
            elif type_node.name.lower() == "realt":
                var_type = VarType.FLOAT
            elif type_node.name.lower() == "chart":
                var_type = VarType.CHAR
            elif type_node.name.lower() == "float":
                var_type = VarType.FLOAT
            else:
                line, column = self.get_location_info(type_node)
                self.report_error(f"Unknown type: {type_node.name}", line, column)
                return None
            
            self.logger.debug(f"Variable type determined: {var_type}")
            return (False, var_type, None)
    
    def analyze_args(self, node: ParseTreeNode) -> Optional[Tuple[int, List[Parameter]]]:
        """
        Analyze an Args node in the parse tree.
        Returns (0, []) if the node does not contain valid arguments.
        """
        self.logger.info(f"Analyzing procedure arguments at depth {self.current_depth}")
        # If the first child is an epsilon production, no formal parameters exist.
        if node.children and node.children[0].name == "ε":
            self.logger.debug("Args has epsilon node, no parameters to process")
            return (0, [])
        if not node.children or not (hasattr(node.children[0], "token") and node.children[0].token):
            self.logger.debug("Args has no valid children, no parameters to process")
            return (0, [])
        
        # Debug the token we're checking
        token_type_name = node.children[0].token.token_type.name
        self.logger.debug(f"First token in Args: {token_type_name}")
        
        if token_type_name != "LPAREN":
            self.logger.debug(f"Args does not start with LPAREN, no parameters")
            return (0, [])
        
        # Otherwise, look for ArgList and process.
        arg_list_node = self.find_child_by_name(node, "ArgList")
        if not arg_list_node:
            self.logger.debug("No ArgList node found in Args")
            return (0, [])
        
        self.logger.debug("Found ArgList node, proceeding to parameter processing")
        return self.analyze_arg_list(arg_list_node)
    
    def analyze_arg_list(self, node: ParseTreeNode) -> Tuple[int, List[Parameter]]:
        """
        Analyze an ArgList node in the parse tree.
        
        Grammar rule: ArgList -> Mode IdentifierList : TypeMark MoreArgs
        
        Args:
            node: The ArgList node to analyze
            
        Returns:
            Tuple of (parameter_count, parameter_list)
        """
        self.logger.debug(f"Starting parameter list analysis for node: {node.name}")
        params = []
        current_node = node
        param_count = 0
        
        while current_node:
            self.logger.debug(f"Processing parameter group #{param_count+1}")
            
            # Process Mode
            mode_node = self.find_child_by_name(current_node, "Mode")
            self.logger.debug(f"Mode node found: {mode_node is not None}")
            param_mode = self.analyze_mode(mode_node) if mode_node else ParameterMode.IN
            self.logger.debug(f"Parameter mode: {param_mode.name}")
            
            # Process IdentifierList
            id_list_node = self.find_child_by_name(current_node, "IdentifierList")
            if not id_list_node:
                self.logger.error("Missing identifier list in parameter group")
                break
            
            identifiers = self.analyze_identifier_list(id_list_node)
            self.logger.debug(f"Found {len(identifiers)} parameter identifiers: {[id.lexeme for id in identifiers]}")
            
            # Verify colon presence
            colon_index = self.find_child_index_by_token_type(current_node, "COLON")
            self.logger.debug(f"Colon index: {colon_index}")
            if colon_index == -1:
                self.logger.error("Missing colon in parameter declaration")
                break
            
            # Process TypeMark
            type_mark_node = self.find_child_by_name(current_node, "TypeMark")
            self.logger.debug(f"TypeMark node found: {type_mark_node is not None}")
            if not type_mark_node:
                self.logger.error("Missing type mark in parameter declaration")
                break
            
            # Get type information
            type_info = self.analyze_type_mark(type_mark_node)
            if not type_info:
                self.logger.error("Failed to determine parameter type")
                break
            
            # Debug the type info
            is_constant, var_type, _ = type_info
            self.logger.debug(f"Parameter type info: constant={is_constant}, type={var_type}")
            
            if is_constant:
                self.logger.error("Constants cannot be used as parameters")
                break
            
            # Process each identifier
            for identifier in identifiers:
                existing_entry = self.symbol_table.lookup(identifier.lexeme, self.current_depth)
                if existing_entry:
                    self.logger.error(f"Duplicate parameter name: '{identifier.lexeme}'")
                    continue
                
                self.logger.debug(f"Creating parameter: {identifier.lexeme}, type={var_type.name}, mode={param_mode.name}")
                param = Parameter(var_type, param_mode)
                params.append(param)
                
                # Insert into symbol table
                entry = self.symbol_table.insert(identifier.lexeme, identifier.token_type, self.current_depth)
                self.logger.debug(f"Inserted parameter '{identifier.lexeme}' into symbol table at depth {self.current_depth}")
                
                # Set variable info
                size = self.type_sizes.get(var_type, 0)
                entry.set_variable_info(var_type, self.current_offset, size)
                entry.entry_type = EntryType.VARIABLE
                
                # Update offset
                self.current_offset += size
                self.depth_offsets[self.current_depth] = self.current_offset
                self.logger.debug(f"Updated offset to {self.current_offset} after parameter {identifier.lexeme}")
            
            # Check for more arguments
            more_args_node = self.find_child_by_name(current_node, "MoreArgs")
            self.logger.debug(f"MoreArgs node found: {more_args_node is not None}")
            
            if more_args_node and more_args_node.children:
                # Debug what we found in MoreArgs
                if hasattr(more_args_node.children[0], "token") and more_args_node.children[0].token:
                    token = more_args_node.children[0].token
                    token_type_name = token.token_type.name
                    self.logger.debug(f"First token in MoreArgs: '{token.lexeme}', type={token_type_name}")
                    
                    if token_type_name == "SEMICOLON":
                        self.logger.debug("Found semicolon, looking for next parameter group")
                        next_arg_list = self.find_child_by_name(more_args_node, "ArgList")
                        if next_arg_list:
                            self.logger.debug("Found next parameter group, continuing")
                            current_node = next_arg_list
                            param_count += 1
                            continue
                        else:
                            self.logger.debug("No ArgList found after semicolon")
                else:
                    self.logger.debug("MoreArgs first child has no token")
            
            self.logger.debug("No more parameter groups, breaking loop")
            break
        
        self.logger.info(f"Parameter analysis complete. Found {len(params)} parameters")
        return (len(params), params)
    
    def analyze_mode(self, node: ParseTreeNode) -> ParameterMode:
        """
        Analyze a Mode node in the parse tree.
        
        Grammar rule: Mode -> in | out | inout | ε
        
        Args:
            node: The Mode node to analyze
            
        Returns:
            Parameter mode (IN, OUT, or INOUT)
        """
        self.logger.debug(f"Analyzing parameter mode for node: {node.name if node else 'None'}")
        if not node or not node.children:
            self.logger.debug("No mode specified, using default IN mode")
            return ParameterMode.IN
        
        mode_token = node.children[0].token
        token_type_name = mode_token.token_type.name
        self.logger.debug(f"Mode token: {mode_token.lexeme}, type={token_type_name}")
        
        if token_type_name == "IN":
            self.logger.debug("Mode determined: IN")
            return ParameterMode.IN
        elif token_type_name == "OUT":
            self.logger.debug("Mode determined: OUT")
            return ParameterMode.OUT
        elif token_type_name == "INOUT":
            self.logger.debug("Mode determined: INOUT")
            return ParameterMode.INOUT
        else:
            self.logger.error(f"Invalid parameter mode: {mode_token.lexeme}")
            return ParameterMode.IN
    
    def analyze_procedures(self, node: ParseTreeNode) -> bool:
        """
        Analyze a Procedures node in the parse tree.
        
        Grammar rule: Procedures -> Prog Procedures | ε
        
        Args:
            node: The Procedures node to analyze
            
        Returns:
            True if analysis succeeded, False if critical errors were found
        """
        self.logger.info(f"Analyzing nested procedures at depth {self.current_depth}")
        if not node or not node.children:
            # No nested procedures (epsilon production)
            return True
        
        # Process the first Prog node (nested procedure)
        prog_node = self.find_child_by_name(node, "Prog")
        if prog_node:
            self.logger.debug(f"Found nested procedure to analyze")
            if not self.analyze_prog(prog_node):
                return False
        
        # Process the rest of the procedures (if any)
        next_procedures = self.find_child_by_name(node, "Procedures")
        if next_procedures and next_procedures.children:
            return self.analyze_procedures(next_procedures)
        
        self.logger.debug("Finished analyzing nested procedures")
        return True
    
    def analyze_seq_of_statements(self, node: ParseTreeNode) -> bool:
        """
        Analyze a SeqOfStatements node in the parse tree.
        
        Grammar rule: SeqOfStatements -> Statement ; SeqOfStatements | ε
        
        Args:
            node: The SeqOfStatements node to analyze
            
        Returns:
            True if analysis succeeded, False if critical errors were found
        """
        self.logger.info(f"Analyzing statements at depth {self.current_depth}")
        if not node or not node.children:
            # Empty sequence of statements (epsilon production)
            return True
        
        # Process the first statement
        statement_node = self.find_child_by_name(node, "Statement")
        if statement_node:
            self.analyze_statement(statement_node)
        
        # Process the rest of the statements (if any)
        semicolon_index = self.find_child_index_by_token_type(node, "SEMICOLON")
        if semicolon_index != -1 and semicolon_index < len(node.children) - 1:
            next_seq_of_statements = node.children[semicolon_index + 1]
            if next_seq_of_statements.name == "SeqOfStatements":
                return self.analyze_seq_of_statements(next_seq_of_statements)
        
        self.logger.debug("Completed statement analysis")
        return True
    
    def analyze_statement(self, node: ParseTreeNode) -> bool:
        """
        Analyze a Statement node in the parse tree.
        
        Grammar rule: Statement -> Assignment | ProcedureCall | IfStatement | LoopStatement
        
        Args:
            node: The Statement node to analyze
            
        Returns:
            True if analysis succeeded, False if critical errors were found
        """
        # Placeholder for statement analysis logic
        return True
    
    def print_symbol_table(self, depth: Optional[int] = None) -> None:
        """
        Print the contents of the symbol table at the specified depth.
        If depth is None, print the entire symbol table.
        
        Args:
            depth: The lexical scope depth to print, or None for all depths
        """
        if depth is not None:
            self.logger.info(f"Printing symbol table for depth {depth}")
            entries = self.symbol_table.writeTable(depth)
            self._print_entries_table(entries, depth)
        else:
            self.logger.info("Printing full symbol table")
            # Find all depths in the symbol table
            depths = set()
            for i in range(self.symbol_table.table_size):
                entry = self.symbol_table.table[i]
                while entry is not None:
                    depths.add(entry.depth)
                    entry = entry.next
            
            # Print entries for each depth
            for d in sorted(depths):
                entries = self.symbol_table.writeTable(d)
                self._print_entries_table(entries, d)
        
        self.logger.debug("Symbol table printing complete")
    
    def _print_entries_table(self, entries: Dict[str, TableEntry], depth: int) -> None:
        """
        Print a nicely formatted table of symbol table entries for a specific depth.
        
        Args:
            entries: Dictionary mapping lexemes to their table entries
            depth: The depth these entries are from
        """
        self.logger.debug(f"Formatting symbol table for depth {depth}, {len(entries)} entries")
        if not entries:
            print(f"\nNo entries at depth {depth}")
            return
        
        print(f"\nEntries at depth {depth}:")
        
        # Create a pretty table
        table = PrettyTable()
        table.field_names = ["Lexeme", "Type", "Data Type", "Size", "Offset/Value", "Other Info"]
        
        for lexeme, entry in sorted(entries.items()):
            entry_type = entry.entry_type.name if entry.entry_type else "UNKNOWN"
            
            if entry.entry_type == EntryType.VARIABLE:
                data_type = entry.var_type.name if entry.var_type else "UNKNOWN"
                size = str(entry.size) if entry.size is not None else "-"
                offset = str(entry.offset) if entry.offset is not None else "-"
                other = "-"
            elif entry.entry_type == EntryType.CONSTANT:
                data_type = entry.const_type.name if entry.const_type else "UNKNOWN"
                size = "-"
                offset = str(entry.const_value) if entry.const_value is not None else "-"
                other = "-"
            elif entry.entry_type == EntryType.PROCEDURE:
                data_type = entry.return_type.name if entry.return_type else "VOID"
                size = str(entry.local_size) if entry.local_size is not None else "-"
                offset = "-"
                params = [str(p) for p in entry.param_list] if entry.param_list else []
                other = f"Params: {', '.join(params)}" if params else "-"
            else:
                data_type = "-"
                size = "-"
                offset = "-"
                other = "-"
                
            table.add_row([lexeme, entry_type, data_type, size, offset, other])
        
        print(table)
    
    def print_errors(self) -> None:
        """
        Print all semantic errors in a formatted table.
        """
        if not self.errors:
            print("\nNo semantic errors detected.")
            return
            
        print("\nSemantic Errors:")
        
        table = PrettyTable()
        table.field_names = ["Line", "Column", "Error Message"]
        
        for error in self.errors:
            line = error.get("line", "-")
            column = error.get("column", "-")
            message = error.get("message", "Unknown error")
            table.add_row([line, column, message])
            
        print(table)
    
    def report_error(self, message: str, line: int = 0, column: int = 0) -> None:
        """
        Report a semantic error.
        
        Args:
            message: The error message
            line: The line number where the error occurred
            column: The column number where the error occurred
        """
        error = {
            "message": message,
            "line": line,
            "column": column
        }
        
        self.errors.append(error)
        self.logger.error(f"Semantic error at line {line}, column {column}: {message}")
        
        if self.stop_on_error:
            print(f"Semantic error at line {line}, column {column}: {message}")
            response = input("Continue analysis? (y/n): ")
            if response.lower() != 'y':
                raise Exception(f"Analysis stopped due to error: {message}")
    
    def get_location_info(self, node_or_token) -> tuple:
        """
        Extract line and column information from a node or token.
        
        Args:
            node_or_token: Either a ParseTreeNode or Token object
        
        Returns:
            Tuple of (line_number, column_number)
        """
        if hasattr(node_or_token, "token") and node_or_token.token:
            # It's a node with a token
            return (node_or_token.token.line_number, node_or_token.token.column_number)
        elif hasattr(node_or_token, "line_number") and hasattr(node_or_token, "column_number"):
            # It's a token
            return (node_or_token.line_number, node_or_token.column_number)
        else:
            # No location info available
            return (0, 0)
    
    def find_child_by_name(self, node: ParseTreeNode, name: str) -> Optional[ParseTreeNode]:
        """
        Helper method to find a child node by name.
        
        Args:
            node: The parent node
            name: The name of the child node to find
            
        Returns:
            The child node if found, None otherwise
        """
        if not node or not node.children:
            return None
            
        for child in node.children:
            if child.name == name:
                return child
                
        return None
    
    def find_child_index_by_token_type(self, node: ParseTreeNode, token_type: str) -> int:
        """
        Helper method to find the index of a child node by token type.
        
        Args:
            node: The parent node
            token_type: The token type to search for
            
        Returns:
            The index of the child if found, -1 otherwise
        """
        if not node or not node.children:
            return -1
            
        for i, child in enumerate(node.children):
            if child.token and child.token.token_type.name == token_type:
                return i
                
        return -1


# Example usage
if __name__ == "__main__":
    # This would be used for testing the module independently
    print("SemanticAnalyzer module - integrate with JohnA5.py driver program")
